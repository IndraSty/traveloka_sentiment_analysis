{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "LGEfjoE-B14G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "A2m7ePrUEEEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages\n",
        "\n",
        "Import all essential libraries and frameworks for comprehensive text preprocessing, including pandas for data manipulation, numpy for numerical operations, matplotlib/seaborn for visualization, NLTK for natural language processing, scikit-learn for text vectorization, and specialized libraries for language detection and text cleaning operations.\n"
      ],
      "metadata": {
        "id": "xGtUrwgg4crs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8FXFpuCmrQO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "\n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "import re\n",
        "import string\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data\n",
        "\n",
        "In this section you are required to load data from local and then extract it to the storage directory.\n"
      ],
      "metadata": {
        "id": "Dpk9mJUy40HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv('traveloka_assessment.csv')"
      ],
      "metadata": {
        "id": "_0C8DMPom8YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.head(10)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YqHwQLtKnC72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe['reviewCreatedVersion'].drop_duplicates()"
      ],
      "metadata": {
        "id": "eAhgqwxE8ql-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Conduct initial data exploration and preparation steps including checking data types, identifying missing values, examining data distribution, and understanding the overall structure of the dataset. This foundational analysis guides the subsequent preprocessing strategy."
      ],
      "metadata": {
        "id": "3OH_OI6Y8XEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.info()"
      ],
      "metadata": {
        "id": "1ZIHjKSSnlva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe.dtypes"
      ],
      "metadata": {
        "id": "T59vF62coOL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataframe.shape)\n",
        "print(\"\\n Check Is Null\")\n",
        "print(dataframe.isnull().sum())\n",
        "print(\"\\n Check Duplicated\")\n",
        "print(dataframe.duplicated().sum())"
      ],
      "metadata": {
        "id": "JHElYlXdnJ1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datacheck = ['content', 'score', 'thumbsUpCount']\n",
        "dataframe[datacheck].head(10)"
      ],
      "metadata": {
        "id": "M5_i7mxGnpJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Data (Text Normalization)\n",
        "\n",
        "Perform the first phase of data cleaning focusing on basic text normalization. This includes handling missing values, removing duplicates, standardizing text encoding, and addressing any structural inconsistencies in the dataset that could affect downstream processing."
      ],
      "metadata": {
        "id": "KdkUEg1MABEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deteksi bahasa untuk setiap review\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        return \"unknown\""
      ],
      "metadata": {
        "id": "ZduL_gQ6AH6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe['language'] = dataframe['content'].apply(detect_language)"
      ],
      "metadata": {
        "id": "ne-jDzV_Bgpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe['language'].value_counts()"
      ],
      "metadata": {
        "id": "P2MUh2OdBnvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove that languange non 'id'\n",
        "dataframe = dataframe[dataframe['language'] == 'id'].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "yAWPDAKsC_MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataframe.copy()\n",
        "df = df[['content', 'score']]\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "K1cQfpj4DSRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create labeling function\n",
        "def label_score(score):\n",
        "    if score >= 4:\n",
        "        return 'positive'\n",
        "    elif score == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'negative'"
      ],
      "metadata": {
        "id": "dugJy7AnDgYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'] = df['score'].apply(label_score)\n",
        "df['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "U_Yt4QQED2Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning Data (basic version + Stopwords + Stemming)\n",
        "\n",
        "Implement comprehensive text cleaning pipeline that includes:\n",
        "\n",
        "- <b>Basic Cleaning:</b> Remove special characters, URLs, email addresses, and unwanted symbols\n",
        "- <b>Stopwords Removal:</b> Eliminate common words that don't contribute to sentiment or meaning\n",
        "- <b>Stemming:</b> Reduce words to their root form using algorithms like Porter Stemmer to normalize word variations and reduce dimensionality\n",
        "\n",
        "Key Features:\n",
        "\n",
        "- <b>Case normalization </b> (converting to lowercase)\n",
        "- <b>Punctuation removal and handling</b>\n",
        "- <b>Number and digit processing</b>\n",
        "- <b>HTML tag removal if present</b>\n",
        "- <b>Whitespace normalization</b>\n",
        "- <b>Language-specific stopword filtering</b>\n",
        "- <b>Root word extraction through stemming algorithms</b>"
      ],
      "metadata": {
        "id": "e4klTmRYESXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize stemmer and stopword remover\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "stop_factory = StopWordRemoverFactory()\n",
        "stopwords = set(stop_factory.get_stop_words())"
      ],
      "metadata": {
        "id": "DAyPvxbZD-d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning function\n",
        "def clean_text(text):\n",
        "    # Remove url\n",
        "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Hilangkan angka dan tanda baca\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # manual tokenize\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "    # stemming\n",
        "    text = ' '.join([stemmer.stem(word) for word in tokens])\n",
        "    return text"
      ],
      "metadata": {
        "id": "NGiGrl9ZEula"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "SNXuq02BUcpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_content'] = df['content'].progress_apply(clean_text)"
      ],
      "metadata": {
        "id": "dKAmIWdTGd2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean_content'].head(10)"
      ],
      "metadata": {
        "id": "rU9KJF56oOIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Clean Data"
      ],
      "metadata": {
        "id": "aGZMXDMA7cCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('traveloka_clean.csv', index=False)"
      ],
      "metadata": {
        "id": "cFcCZInGoSs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleandf = pd.read_csv('traveloka_clean.csv')"
      ],
      "metadata": {
        "id": "YBU2UYbsofh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleandf.head()"
      ],
      "metadata": {
        "id": "RTWcM5EQohxC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}