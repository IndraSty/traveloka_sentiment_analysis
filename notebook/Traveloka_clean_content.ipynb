{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGEfjoE-B14G"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2m7ePrUEEEZ"
      },
      "outputs": [],
      "source": [
        "!pip install Sastrawi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGtUrwgg4crs"
      },
      "source": [
        "## Importing Packages\n",
        "\n",
        "Import all essential libraries and frameworks for comprehensive text preprocessing, including pandas for data manipulation, numpy for numerical operations, matplotlib/seaborn for visualization, NLTK for natural language processing, scikit-learn for text vectorization, and specialized libraries for language detection and text cleaning operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8FXFpuCmrQO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "\n",
        "from langdetect import detect\n",
        "from langdetect.lang_detect_exception import LangDetectException\n",
        "\n",
        "import re\n",
        "import string\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpk9mJUy40HI"
      },
      "source": [
        "## Loading Data\n",
        "\n",
        "In this section you are required to load data from local and then extract it to the storage directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0C8DMPom8YX"
      },
      "outputs": [],
      "source": [
        "dataframe = pd.read_csv('traveloka_assessment.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YqHwQLtKnC72"
      },
      "outputs": [],
      "source": [
        "dataframe.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAhgqwxE8ql-"
      },
      "outputs": [],
      "source": [
        "dataframe['reviewCreatedVersion'].drop_duplicates()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OH_OI6Y8XEb"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Conduct initial data exploration and preparation steps including checking data types, identifying missing values, examining data distribution, and understanding the overall structure of the dataset. This foundational analysis guides the subsequent preprocessing strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZIHjKSSnlva"
      },
      "outputs": [],
      "source": [
        "dataframe.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T59vF62coOL2"
      },
      "outputs": [],
      "source": [
        "dataframe.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHElYlXdnJ1n"
      },
      "outputs": [],
      "source": [
        "print(dataframe.shape)\n",
        "print(\"\\n Check Is Null\")\n",
        "print(dataframe.isnull().sum())\n",
        "print(\"\\n Check Duplicated\")\n",
        "print(dataframe.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5_i7mxGnpJ-"
      },
      "outputs": [],
      "source": [
        "datacheck = ['content', 'score', 'thumbsUpCount']\n",
        "dataframe[datacheck].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdkUEg1MABEA"
      },
      "source": [
        "## Clean Data (Text Normalization)\n",
        "\n",
        "Perform the first phase of data cleaning focusing on basic text normalization. This includes handling missing values, removing duplicates, standardizing text encoding, and addressing any structural inconsistencies in the dataset that could affect downstream processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZduL_gQ6AH6f"
      },
      "outputs": [],
      "source": [
        "# Deteksi bahasa untuk setiap review\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except LangDetectException:\n",
        "        return \"unknown\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne-jDzV_Bgpk"
      },
      "outputs": [],
      "source": [
        "dataframe['language'] = dataframe['content'].apply(detect_language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2MUh2OdBnvk"
      },
      "outputs": [],
      "source": [
        "dataframe['language'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAWPDAKsC_MZ"
      },
      "outputs": [],
      "source": [
        "# remove that languange non 'id'\n",
        "dataframe = dataframe[dataframe['language'] == 'id'].reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1cQfpj4DSRt"
      },
      "outputs": [],
      "source": [
        "df = dataframe.copy()\n",
        "df = df[['content', 'score']]\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dugJy7AnDgYC"
      },
      "outputs": [],
      "source": [
        "# create labeling function\n",
        "def label_score(score):\n",
        "    if score >= 4:\n",
        "        return 'positive'\n",
        "    elif score == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'negative'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_Yt4QQED2Xy"
      },
      "outputs": [],
      "source": [
        "df['sentiment'] = df['score'].apply(label_score)\n",
        "df['sentiment'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4klTmRYESXN"
      },
      "source": [
        "## Cleaning Data (basic version + Stopwords + Stemming)\n",
        "\n",
        "Implement comprehensive text cleaning pipeline that includes:\n",
        "\n",
        "- <b>Basic Cleaning:</b> Remove special characters, URLs, email addresses, and unwanted symbols\n",
        "- <b>Stopwords Removal:</b> Eliminate common words that don't contribute to sentiment or meaning\n",
        "- <b>Stemming:</b> Reduce words to their root form using algorithms like Porter Stemmer to normalize word variations and reduce dimensionality\n",
        "\n",
        "Key Features:\n",
        "\n",
        "- <b>Case normalization </b> (converting to lowercase)\n",
        "- <b>Punctuation removal and handling</b>\n",
        "- <b>Number and digit processing</b>\n",
        "- <b>HTML tag removal if present</b>\n",
        "- <b>Whitespace normalization</b>\n",
        "- <b>Language-specific stopword filtering</b>\n",
        "- <b>Root word extraction through stemming algorithms</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAyPvxbZD-d3"
      },
      "outputs": [],
      "source": [
        "# initialize stemmer and stopword remover\n",
        "stemmer = StemmerFactory().create_stemmer()\n",
        "stop_factory = StopWordRemoverFactory()\n",
        "stopwords = set(stop_factory.get_stop_words())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to load slang words\n",
        "def load_slang_dict(file_path='slangs.txt'):\n",
        "    \"\"\"Load slang dictionary from file\"\"\"\n",
        "    slang_dict = {}\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='replace') as file:\n",
        "            for line in file:\n",
        "                line = line.strip()\n",
        "                if ':' in line:\n",
        "                    parts = line.split(':', 1)\n",
        "                    slang = parts[0].strip().lower()\n",
        "                    formal = parts[1].strip().lower()\n",
        "                    slang_dict[slang] = formal\n",
        "        print(f\"Loaded {len(slang_dict)} slang words from {file_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {file_path} not found. Slang normalization will be skipped.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading slang dictionary: {e}\")\n",
        "    return slang_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load slang dictionary\n",
        "slang_dict = load_slang_dict('slangs.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_slang(text, slang_dict):\n",
        "    \"\"\"Replace slang words with formal words\"\"\"\n",
        "    if not slang_dict:\n",
        "        return text\n",
        "\n",
        "    words = text.split()\n",
        "    normalized_words = []\n",
        "\n",
        "    for word in words:\n",
        "        # Check if word exists in slang dictionary\n",
        "        if word.lower() in slang_dict:\n",
        "            formal_word = slang_dict[word.lower()]\n",
        "            # Only add if formal word is not empty\n",
        "            if formal_word.strip():\n",
        "                normalized_words.append(formal_word)\n",
        "        else:\n",
        "            normalized_words.append(word)\n",
        "\n",
        "    return ' '.join(normalized_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGiGrl9ZEula"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Comprehensive text cleaning with slang normalization\n",
        "    \"\"\"\n",
        "    # Remove mentions, hashtags, RT\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+|#[A-Za-z0-9_]+|RT\\s+', '', text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'[0-9]+', '', text)\n",
        "\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Normalize slang words BEFORE removing punctuation\n",
        "    text = normalize_slang(text, slang_dict)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove non-alphabetic characters, keep only letters and spaces\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize and remove stopwords + short words\n",
        "    tokens = [word for word in text.split()\n",
        "              if word not in stopwords and len(word) > 3]\n",
        "\n",
        "    # Stemming\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(stemmed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNXuq02BUcpb"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKAmIWdTGd2Q"
      },
      "outputs": [],
      "source": [
        "df['clean_content'] = df['content'].progress_apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU9KJF56oOIq"
      },
      "outputs": [],
      "source": [
        "df['clean_content'].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGZMXDMA7cCc"
      },
      "source": [
        "## Save Clean Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFcCZInGoSs2"
      },
      "outputs": [],
      "source": [
        "df.to_csv('traveloka_clean.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBU2UYbsofh-"
      },
      "outputs": [],
      "source": [
        "cleandf = pd.read_csv('traveloka_clean.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTWcM5EQohxC"
      },
      "outputs": [],
      "source": [
        "cleandf.head()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
