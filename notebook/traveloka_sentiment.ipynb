{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Traveloka Sentiment Analysis\n",
        "\n",
        "![Traveloka Logo](https://console.kr-asia.com/wp-content/uploads/2020/12/traveloka.jpg)\n",
        "\n",
        "![Python](https://img.shields.io/badge/Python-3.12-blue)\n",
        "![Machine Learning](https://img.shields.io/badge/Machine%20Learning-Sentiment%20Analysis-orange)\n",
        "![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-Modeling-yellow)\n",
        "![Status](https://img.shields.io/badge/Status-Completed-brightgreen)\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ‘¤ My Identity\n",
        "- **Name** : Indra Styawan  \n",
        "- **Role** : Data Science  \n",
        "- **Domicile** : Yogyakarta, Indonesia  \n",
        "- **Email** : indrastyawan0925@gmail.com  \n",
        "- **LinkedIn** : www.linkedin.com/in/indrastyawan25\n",
        "\n",
        "---\n",
        "\n",
        "<h1><center>ğŸ“ˆ Analysis of App's Review on Traveloka Application </center></h1>\n",
        "\n",
        "### ğŸ“ Introduction\n",
        "<p align=\"justify\">The application review analysis program in the Traveloka application is a machine learning application that aims to analyze reviews in the Play Store on the Traveloka application. The general purpose of this program is to find out which reviews are positive, negative, or neutral, so that it can be analyzed how the public responds to the Traveloka application that can be used to book flights, hotels, and various travel-related services.</p>\n",
        "\n",
        "### ğŸ¯ Objective\n",
        "<p align=\"justify\">The process of analyzing and evaluating reviews or feelings expressed by Traveloka application users in their reviews that touch on Traveloka application services on the Play Store.</p>\n",
        "\n",
        "### ğŸ” Process\n",
        "- ğŸ“¥ **Data Collection**: Data collection was carried out by collecting data on the Play Store with the Traveloka application ID using the Google-Play-Scraper library.  \n",
        "- ğŸ§¹ **Data Preprocessing**: CleaningText, casefoldingText, tokenizationText, filteringText, stemming/lemmatization, and toSentence.  \n",
        "- ğŸ·ï¸ **Data Labeling**: The process of assigning a category or label to each data entry based on available information.  \n",
        "- â˜ï¸ **Label Exploration**: This visualization uses WordCloud.  \n",
        "- âœ‚ï¸ **Dataset Splitting**: Splitting the dataset into training, validation, and test sets for the model training process.  \n",
        "- ğŸ§  **Model Building**: Build a classification model using a random forest, support vector machines, gradient boosting machines, and XGBoost.  \n",
        "- ğŸ‹ï¸â€â™‚ï¸ **Model Training**: Training a model on a training dataset by optimizing its parameters and weights so that it can recognize patterns in text.  \n",
        "- âœ… **Model Validation**: Validate the model on the validation dataset to measure its performance and prevent overfitting.  \n",
        "- ğŸ“Š **Evaluation and Tuning**: Evaluate the model on the test dataset and adjust parameters if necessary.\n"
      ],
      "metadata": {
        "id": "zCc8ZbOge3Sd"
      },
      "id": "zCc8ZbOge3Sd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Importing Packages\n",
        "\n",
        "---\n",
        "    \n",
        "| âš¡ Description: Importing Packages âš¡ |\n",
        "| :--------------------------- |\n",
        "| In this section the required packages are imported, and briefly discuss, the libraries that will be used throughout the analysis and modelling. |"
      ],
      "metadata": {
        "id": "qxT_d0_dh99_"
      },
      "id": "qxT_d0_dh99_"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyY9nHPAhc2a",
        "outputId": "07fba748-7d8a-45bd-bd90-63851f10e8f3"
      },
      "id": "fyY9nHPAhc2a",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m204.8/209.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library for data cleaning\n",
        "import pandas as pd  # Pandas for data manipulation and analysis\n",
        "pd.options.mode.chained_assignment = None  # Disable chained assignment warning\n",
        "import numpy as np  # NumPy for numerical computation\n",
        "seed = 0\n",
        "np.random.seed(seed)  # Set seed for reproducibility\n",
        "import re  # Module for working with regular expressions\n",
        "import string  # Contains string constants such as punctuation marks\n",
        "import nltk  # Import NLTK (Natural Language Toolkit) library\n",
        "nltk.download('punkt')  # Download dataset required for text tokenization\n",
        "nltk.download('stopwords')  # Download dataset containing stopword lists in various languages\n",
        "from nltk.tokenize import word_tokenize  # Text tokenization\n",
        "from nltk.corpus import stopwords  # Stopword list in text\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory  # Stemming (removing word affixes) for Indonesian language\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory  # Remove stopwords in Indonesian language\n",
        "import csv\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# Import library for visualization\n",
        "import matplotlib.pyplot as plt  # Matplotlib for data visualization\n",
        "import seaborn as sns  # Seaborn for statistical data visualization and style setting\n",
        "from wordcloud import WordCloud  # Create a word cloud visualization from text\n",
        "\n",
        "# Import library for preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import library for processing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import xgboost as xgb\n"
      ],
      "metadata": {
        "id": "QVHss1bZiBb9",
        "outputId": "6da32f14-aea5-4603-a6d7-ed4b89334437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "QVHss1bZiBb9",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NbEKD7AZiQNB"
      },
      "id": "NbEKD7AZiQNB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}