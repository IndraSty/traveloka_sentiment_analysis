{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages\n",
        "\n",
        "---\n",
        "    \n",
        "| ‚ö° Description: Importing Packages ‚ö° |\n",
        "| :--------------------------- |\n",
        "| In this section the required packages are imported, and briefly discuss, the libraries that will be used throughout the analysis and modelling. |"
      ],
      "metadata": {
        "id": "AZqhZEW0nqD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgfuAfCRMlcf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data\n",
        "\n",
        "---\n",
        "\n",
        "In this section you are required to load data from local and then extract it to the storage directory.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "FPafV6mbnw-J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWaIUH_DPYa_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('traveloka_clean.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM1EOO7KPaLt"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visualization Distribution\n",
        "\n",
        "---\n",
        "\n",
        "Create data distribution visualizations to understand the distribution of sentiment classes (positive, negative, neutral) in the dataset. This analysis is crucial for identifying potential class imbalance and planning appropriate modeling strategies.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "l2SXAA4an57y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpDM_uVdPd5e"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='score', data=df)\n",
        "plt.title('Distribution of Score Labels')\n",
        "plt.xlabel('Score')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EjCmEwcPhJr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='sentiment', data=df)\n",
        "plt.title('Distribution of Sentiment Labels')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "403PD_pT70xQ"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtBZbpJm6wJt"
      },
      "source": [
        "#### Label Encoder for Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['clean_content']\n",
        "y = df['sentiment']"
      ],
      "metadata": {
        "id": "Lqx8-w1lnFDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utQYBYIX6p6S"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEngX_LS62ZR"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n=== TARGET VARIABLE ENCODING ===\")\n",
        "print(\"Label mapping:\")\n",
        "for i, class_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"{class_name} -> {i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDh6SCKA66_Z"
      },
      "source": [
        "#### Split Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvS5l0MM64YS"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X,\n",
        "    y_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_encoded\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6tL_iL37hrQ"
      },
      "outputs": [],
      "source": [
        "print(f\"\\n=== DATA SPLITTING ===\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Testing labels shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDMsG-Vu7jne"
      },
      "outputs": [],
      "source": [
        "# Display class distribution in train and test sets\n",
        "print(f\"\\nTraining set class distribution:\")\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "for class_idx, count in zip(unique, counts):\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    print(f\"{class_name}: {count} ({count/len(y_train)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj_7Bn-q7rTv"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nTesting set class distribution:\")\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "for class_idx, count in zip(unique, counts):\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    print(f\"{class_name}: {count} ({count/len(y_test)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSAiEgMM5ddO"
      },
      "source": [
        "### TF-IDF Vectorizer\n",
        "\n",
        "---\n",
        "\n",
        "Implement Term Frequency-Inverse Document Frequency (TF-IDF) to convert text into numerical representations. TF-IDF assigns higher weights to words that are rare but important in the context of specific documents.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJYaLopuPju2"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(\n",
        "    max_features=1000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=5,\n",
        "    max_df=0.75,\n",
        "    lowercase=True,\n",
        "    stop_words=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmTHCWZ96T55"
      },
      "outputs": [],
      "source": [
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VBBt0eI6gtO"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nTF-IDF matrix shape: {X_tfidf.shape}\")\n",
        "print(f\"Number of features created: {len(tfidf_vectorizer.get_feature_names_out())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgGdsQw66jH2"
      },
      "outputs": [],
      "source": [
        "# Display some sample features\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "print(f\"\\nSample features (first 20):\")\n",
        "for i, feature in enumerate(feature_names[:20]):\n",
        "    print(f\"{i+1:2d}. {feature}\")\n",
        "\n",
        "print(f\"\\nSample features (last 20):\")\n",
        "for i, feature in enumerate(feature_names[-20:]):\n",
        "    print(f\"{len(feature_names)-19+i:2d}. {feature}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTsSermy8rLd"
      },
      "source": [
        "### Handling Imbalanced Data\n",
        "\n",
        "---\n",
        "\n",
        "Address data imbalance using techniques such as SMOTE (Synthetic Minority Oversampling Technique), undersampling, or class weighting to ensure the model can predict all classes effectively.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Class Distribution\n",
        "\n",
        "---\n",
        "\n",
        "Analyze the distribution of sentiment classes in the dataset to identify the severity of class imbalance. This step visualizes the count of samples for each class and calculates the imbalance ratio to determine appropriate resampling strategies.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tOxvfjwJuW9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTJbfHJc7uqo"
      },
      "outputs": [],
      "source": [
        "print(\"Original class distributiin: \")\n",
        "original_distribution = Counter(y_train)\n",
        "for class_idx, count in original_distribution.items():\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    print(f\"{class_name}: {count} ({count/len(y_train)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B0UU2989HXU"
      },
      "outputs": [],
      "source": [
        "# calculate imbalanced ratio\n",
        "max_count = max(original_distribution.values())\n",
        "min_count = min(original_distribution.values())\n",
        "imbalanced_ratio = max_count / min_count\n",
        "print(f\"\\nImbalanced ratio: {imbalanced_ratio:.2f}:1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SMOTE\n",
        "\n",
        "---\n",
        "\n",
        "Apply Synthetic Minority Oversampling Technique (SMOTE) to generate synthetic samples for minority classes. SMOTE creates new instances by interpolating between existing minority class samples, effectively balancing the dataset without simply duplicating existing data points.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2uzv66pfufVF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQRkgZMD9eII"
      },
      "outputs": [],
      "source": [
        "# initialize SMOTE\n",
        "smote = SMOTE(\n",
        "    sampling_strategy='auto',\n",
        "    random_state=42,\n",
        "    k_neighbors=5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYcyQbwa9zDr"
      },
      "outputs": [],
      "source": [
        "# applying SMOTE to training data\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCCGL0os-CGJ"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nResampling completed!\")\n",
        "print(f\"Original training shape: {X_train_tfidf.shape}\")\n",
        "print(f\"Balanced training shape: {X_train_smote.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rE89jxz-Fq5"
      },
      "outputs": [],
      "source": [
        "# Check new class distribution\n",
        "print(f\"\\nBalanced class distribution:\")\n",
        "balanced_distribution = Counter(y_train_smote)\n",
        "for class_idx, count in balanced_distribution.items():\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    print(f\"{class_name}: {count} ({count/len(y_train_smote)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eNI5P_L-J01"
      },
      "outputs": [],
      "source": [
        "# visualize before and after balance\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
        "# before SMOTE\n",
        "original_labels = [label_encoder.inverse_transform([idx])[0] for idx in original_distribution.keys()]\n",
        "original_counts = list(original_distribution.values())\n",
        "\n",
        "ax1.bar(original_labels, original_counts, color=['#1f77b4', '#ff7f03', '#2ca02c'])\n",
        "ax1.set_title('Before SMOTE - Imbalanced Data')\n",
        "ax1.set_xlabel('Sentiment')\n",
        "ax1.set_ylabel('Count')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# add count labels on bars\n",
        "for i, count in enumerate(original_counts):\n",
        "  ax1.text(i, count + max(original_counts)*0.01, str(count), ha='center')\n",
        "\n",
        "# after SMOTE\n",
        "balanced_labels = [label_encoder.inverse_transform([idx])[0] for idx in balanced_distribution.keys()]\n",
        "balanced_counts = list(balanced_distribution.values())\n",
        "\n",
        "ax2.bar(balanced_labels, balanced_counts, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "ax2.set_title('After SMOTE - Balanced Data')\n",
        "ax2.set_xlabel('Sentiment')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# add count labels on bars\n",
        "for i, count in enumerate(balanced_counts):\n",
        "  ax2.text(i, count + max(balanced_counts)*0.01, str(count), ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2-6fjjJG81N"
      },
      "source": [
        "### SMOTETomek\n",
        "\n",
        "---\n",
        "\n",
        "Implement SMOTETomek, a hybrid approach that combines SMOTE oversampling with Tomek links undersampling. This technique first applies SMOTE to increase minority class samples, then removes Tomek links (pairs of samples from different classes that are each other's nearest neighbors) to clean overlapping regions and improve class separation.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khPqYaJVAc_l"
      },
      "outputs": [],
      "source": [
        "smote_tomek = SMOTETomek(\n",
        "    sampling_strategy='auto',\n",
        "    random_state=42,\n",
        "    smote=SMOTE(k_neighbors=5, random_state=42)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a67qiyDGlxA"
      },
      "outputs": [],
      "source": [
        "X_train_smotetomek, y_train_smotetomek = smote_tomek.fit_resample(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY3nXOz-G6Y8"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nSMOTETomek results:\")\n",
        "print(f\"Original training shape: {X_train_tfidf.shape}\")\n",
        "print(f\"SMOTETomek training shape: {X_train_smotetomek.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxNtIkTCHOKd"
      },
      "outputs": [],
      "source": [
        "smotetomek_distribution = Counter(y_train_smotetomek)\n",
        "print(f\"\\nSMOTETomek class distribution:\")\n",
        "for class_idx, count in smotetomek_distribution.items():\n",
        "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
        "    print(f\"{class_name}: {count} ({count/len(y_train_smotetomek)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzO6ZVhoK_xl"
      },
      "source": [
        "### Initialize final train data\n",
        "\n",
        "---\n",
        "\n",
        "Prepare the final balanced training dataset by selecting the best resampling technique based on evaluation metrics. This step finalizes the data preprocessing pipeline and creates the optimized dataset for model training.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7CElEPDLDFS"
      },
      "outputs": [],
      "source": [
        "X_train_final = X_train_smote\n",
        "y_train_final = y_train_smote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuntbADJHSDX"
      },
      "outputs": [],
      "source": [
        "print(\"=== MODEL IMPLEMENTATION ===\")\n",
        "print(f\"Training data shape: {X_train_final.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}\")\n",
        "print(f\"Training started at: {datetime.now().strftime('%H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0C3T5FCULItH"
      },
      "outputs": [],
      "source": [
        "models = {}\n",
        "results = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f2qYI2kK6pv"
      },
      "source": [
        "## Model Implementation\n",
        "\n",
        "---\n",
        "\n",
        "Implement various machine learning algorithms for sentiment classification, including traditional models such as Random Forest, Naive Bayes, SVM, and Logistic Regression.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBiVQ9XkLTm1"
      },
      "source": [
        "### Random Forest\n",
        "\n",
        "---\n",
        "\n",
        "Implement Random Forest classifier, an ensemble method that combines multiple decision trees to improve prediction accuracy and reduce overfitting. This algorithm is particularly effective for text classification as it can handle high-dimensional sparse data and provides feature importance rankings.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH1K5-6RLOq8"
      },
      "outputs": [],
      "source": [
        "rf_start = datetime.now()\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=20,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    class_weight='balanced'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7Uj_8WPLRJZ"
      },
      "outputs": [],
      "source": [
        "rf_model.fit(X_train_final, y_train_final)\n",
        "rf_pred = rf_model.predict(X_test_tfidf)\n",
        "rf_time = (datetime.now() - rf_start).total_seconds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9jEqBLHL9W2"
      },
      "outputs": [],
      "source": [
        "models['Random Forest'] = rf_model\n",
        "results['Random Forest'] = {\n",
        "    'predictions': rf_pred,\n",
        "    'accuracy': accuracy_score(y_test, rf_pred),\n",
        "    'precision': precision_recall_fscore_support(y_test, rf_pred),\n",
        "    'training_time': rf_time,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS151SE9MTdG"
      },
      "outputs": [],
      "source": [
        "print(f\"‚úì Random Forest completed in {rf_time:.2f} seconds\")\n",
        "print(f\"  Accuracy: {results['Random Forest']['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md4kzDgTMXCH"
      },
      "source": [
        "### SVC\n",
        "\n",
        "---\n",
        "\n",
        "Deploy Support Vector Classifier for sentiment classification. SVC finds the optimal hyperplane that separates different sentiment classes with maximum margin, making it robust for high-dimensional text data and effective in handling non-linearly separable data through kernel methods.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbln3pcFMXze"
      },
      "outputs": [],
      "source": [
        "svc_start = datetime.now()\n",
        "svc_model = SVC(\n",
        "    C=1.0,\n",
        "    kernel='rbf',\n",
        "    gamma='scale',\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYoxKtFVMhHz"
      },
      "outputs": [],
      "source": [
        "svc_model.fit(X_train_final, y_train_final)\n",
        "svc_pred = svc_model.predict(X_test_tfidf)\n",
        "svc_time = (datetime.now() - svc_start).total_seconds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMgddk7gMnex"
      },
      "outputs": [],
      "source": [
        "models['SVC'] = svc_model\n",
        "results['SVC'] = {\n",
        "    'predictions': svc_pred,\n",
        "    'accuracy': accuracy_score(y_test, svc_pred),\n",
        "    'precision': precision_recall_fscore_support(y_test, svc_pred),\n",
        "    'training_time': svc_time,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2Ymi5rIQuMa"
      },
      "outputs": [],
      "source": [
        "print(f\"‚úì SVC completed in {svc_time:.2f} seconds\")\n",
        "print(f\"  Accuracy: {results['SVC']['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyLx2swCQ5rM"
      },
      "source": [
        "### Naive Bayes\n",
        "\n",
        "---\n",
        "\n",
        "Implement Naive Bayes classifier, a probabilistic algorithm that applies Bayes' theorem with strong independence assumptions between features. This model is particularly suitable for text classification tasks due to its effectiveness with sparse data and computational efficiency.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyIlgvhmQ4d0"
      },
      "outputs": [],
      "source": [
        "nb_start = datetime.now()\n",
        "nb_model = MultinomialNB(\n",
        "    alpha=1.0,\n",
        "    fit_prior=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIj49JRdRHML"
      },
      "outputs": [],
      "source": [
        "nb_model.fit(X_train_final, y_train_final)\n",
        "nb_pred = nb_model.predict(X_test_tfidf)\n",
        "nb_time = (datetime.now() - nb_start).total_seconds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtEc9R8tRNbL"
      },
      "outputs": [],
      "source": [
        "models['Naive Bayes'] = nb_model\n",
        "results['Naive Bayes'] = {\n",
        "    'predictions': nb_pred,\n",
        "    'accuracy': accuracy_score(y_test, nb_pred),\n",
        "    'precision': precision_recall_fscore_support(y_test, nb_pred),\n",
        "    'training_time': nb_time,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zWvupH-RaAw"
      },
      "outputs": [],
      "source": [
        "print(f\"‚úì Naive Bayes completed in {nb_time:.2f} seconds\")\n",
        "print(f\"  Accuracy: {results['Naive Bayes']['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8ETq2rdRdtn"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "---\n",
        "\n",
        "Apply Logistic Regression for multi-class sentiment classification. This linear model uses the logistic function to model the probability of class membership and provides interpretable coefficients that can help understand which features contribute most to sentiment predictions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhfW3E8oRcEh"
      },
      "outputs": [],
      "source": [
        "lr_start = datetime.now()\n",
        "lr_model = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    solver='liblinear',\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    C=1.0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWypey7oR-f_"
      },
      "outputs": [],
      "source": [
        "lr_model.fit(X_train_final, y_train_final)\n",
        "lr_pred = lr_model.predict(X_test_tfidf)\n",
        "lr_time = (datetime.now() - lr_start).total_seconds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUUwexvkSIFy"
      },
      "outputs": [],
      "source": [
        "models['Logistic Regression'] = lr_model\n",
        "results['Logistic Regression'] = {\n",
        "    'predictions': lr_pred,\n",
        "    'accuracy': accuracy_score(y_test, lr_pred),\n",
        "    'precision': precision_recall_fscore_support(y_test, lr_pred),\n",
        "    'training_time': lr_time,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqH9LQkDSQNM"
      },
      "outputs": [],
      "source": [
        "print(f\"‚úì Logistic Regression completed in {lr_time:.2f} seconds\")\n",
        "print(f\"  Accuracy: {results['Logistic Regression']['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYfyu5HTSWRU"
      },
      "source": [
        "## Summary of All Models\n",
        "\n",
        "---\n",
        "\n",
        "Create a performance summary of all trained models, displaying evaluation metrics such as accuracy, precision, recall, and F1-score for each model. This analysis helps in selecting the best performing model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eep06j8SSly"
      },
      "outputs": [],
      "source": [
        "summary_data = []\n",
        "for model_name, result in results.items():\n",
        "    summary_data.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': result['accuracy'],\n",
        "        'Precision': result['precision'][0],\n",
        "        'Recall': result['precision'][1],\n",
        "        'F1-Score': result['precision'][2],\n",
        "        'Support': result['precision'][3],\n",
        "        'Training Time (s)': result['training_time'],\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zFVNO6_S2I5"
      },
      "outputs": [],
      "source": [
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df = summary_df.sort_values(by='Accuracy', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3lfhx6WS-G_"
      },
      "outputs": [],
      "source": [
        "print(summary_df.to_string(index=False, float_format='%.4f'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ1EUu5pTnL2"
      },
      "source": [
        "### Visualize model Comparison\n",
        "\n",
        "---\n",
        "\n",
        "Create visualizations comparing model performance using bar charts, confusion matrix heatmaps, or ROC curves to provide clear insights into the strengths and weaknesses of each model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBXfUAHITP1g"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# accuracy comparison\n",
        "ax1.bar(summary_df['Model'], summary_df['Accuracy'],\n",
        "        color=['#1f77b4', '#ff7f03', '#2ca02c', '#d62728'])\n",
        "ax1.set_title('Model Accuracy Comparison')\n",
        "ax1.set_xlabel('Model')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_ylim(0, 1)\n",
        "plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# add accuracy values on bars\n",
        "for i, v in enumerate(summary_df['Accuracy']):\n",
        "  ax1.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Training time comparison\n",
        "ax2.bar(summary_df['Model'], summary_df['Training Time (s)'],\n",
        "        color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "ax2.set_title('Training Time Comparison')\n",
        "ax2.set_ylabel('Training Time (seconds)')\n",
        "plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# add time values on bars\n",
        "for i, v in enumerate(summary_df['Training Time (s)']):\n",
        "  ax2.text(i, v + max(summary_df['Training Time (s)']) * 0.01, f'{v:.1f}s',\n",
        "           ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob-h7681Vn80"
      },
      "outputs": [],
      "source": [
        "# Best model identification\n",
        "best_model = summary_df.iloc[0]['Model']\n",
        "best_accuracy = summary_df.iloc[0]['Accuracy']\n",
        "print(f\"\\nüèÜ Best performing model: {best_model} (Accuracy: {best_accuracy:.4f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhmJoafpZ4Mc"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "---\n",
        "\n",
        "Conduct in-depth evaluation of selected models using various evaluation metrics and validation techniques such as cross-validation to ensure model robustness and generalization.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBloriopVuHI"
      },
      "outputs": [],
      "source": [
        "class_names = label_encoder.classes_\n",
        "n_classes = len(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1SrW5a_aJer"
      },
      "outputs": [],
      "source": [
        "print(f\"Classes: {class_names}\")\n",
        "print(f\"Number of classes: {n_classes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ql3mru9aLGG"
      },
      "outputs": [],
      "source": [
        "# Function to plot confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name, class_names):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "              xticklabels=class_names, yticklabels=class_names)\n",
        "  plt.title(f'Confusion Matrix - {model_name}')\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Actual')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aBaQmWJayTY"
      },
      "outputs": [],
      "source": [
        "detailed_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdXmVTLba4pZ"
      },
      "outputs": [],
      "source": [
        "for model_name, result in results.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"DETAILED EVALUATION: {model_name.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    y_pred = result['predictions']\n",
        "\n",
        "    # basic metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Training Time: {result['training_time']:.2f} seconds\")\n",
        "\n",
        "    # Classification report\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "    # confusion matrix\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    cm = plot_confusion_matrix(y_test, y_pred, model_name, class_names)\n",
        "\n",
        "    # per-class metrics\n",
        "    print(f\"\\nPer-Class Metrics:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name:>10}: Precision={precision[i]:.4f}, Recall={recall[i]:.4f}, F1={f1[i]:.4f}, Support={support[i]}\")\n",
        "\n",
        "    # Macro and weighted averages\n",
        "    precision_macro = np.mean(precision)\n",
        "    recall_macro = np.mean(recall)\n",
        "    f1_macro = np.mean(f1)\n",
        "\n",
        "    precision_weighted = np.average(precision, weights=support)\n",
        "    recall_weighted = np.average(recall, weights=support)\n",
        "    f1_weighted = np.average(f1, weights=support)\n",
        "\n",
        "    print(f\"\\nMacro Average:    Precision={precision_macro:.4f}, Recall={recall_macro:.4f}, F1={f1_macro:.4f}\")\n",
        "    print(f\"Weighted Average: Precision={precision_weighted:.4f}, Recall={recall_weighted:.4f}, F1={f1_weighted:.4f}\")\n",
        "\n",
        "    # Store detailed results\n",
        "    detailed_results[model_name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'confusion_matrix': cm,\n",
        "        'per_class_precision': precision,\n",
        "        'per_class_recall': recall,\n",
        "        'per_class_f1': f1,\n",
        "        'support': support\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCkL8uFNd0tT"
      },
      "source": [
        "### Comprehensive Comparison Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3kkA5q1cecV"
      },
      "outputs": [],
      "source": [
        "comparison_data = []\n",
        "for model_name, metrics in detailed_results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': metrics['accuracy'],\n",
        "        'Precision (Macro)': metrics['precision_macro'],\n",
        "        'Recall (Macro)': metrics['recall_macro'],\n",
        "        'F1 (Macro)': metrics['f1_macro'],\n",
        "        'Precision (Weighted)': metrics['precision_weighted'],\n",
        "        'Recall (Weighted)': metrics['recall_weighted'],\n",
        "        'F1 (Weighted)': metrics['f1_weighted'],\n",
        "        'Training Time (s)': results[model_name]['training_time']\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSlrKCtcdxbn"
      },
      "outputs": [],
      "source": [
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(comparison_df.to_string(index=False, float_format='%.4f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zV4cobXNdzq6"
      },
      "outputs": [],
      "source": [
        "# Visualize comprehensive comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0,0].bar(comparison_df['Model'], comparison_df['Accuracy'],\n",
        "              color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[0,0].set_title('Accuracy Comparison')\n",
        "axes[0,0].set_ylabel('Accuracy')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(comparison_df['Accuracy']):\n",
        "    axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# F1 Macro comparison\n",
        "axes[0,1].bar(comparison_df['Model'], comparison_df['F1 (Macro)'],\n",
        "              color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[0,1].set_title('F1 Score (Macro) Comparison')\n",
        "axes[0,1].set_ylabel('F1 Score (Macro)')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(comparison_df['F1 (Macro)']):\n",
        "    axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Precision vs Recall (Macro)\n",
        "axes[1,0].scatter(comparison_df['Recall (Macro)'], comparison_df['Precision (Macro)'],\n",
        "                  s=100, c=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    axes[1,0].annotate(model,\n",
        "                       (comparison_df['Recall (Macro)'].iloc[i],\n",
        "                        comparison_df['Precision (Macro)'].iloc[i]),\n",
        "                       xytext=(5, 5), textcoords='offset points')\n",
        "axes[1,0].set_xlabel('Recall (Macro)')\n",
        "axes[1,0].set_ylabel('Precision (Macro)')\n",
        "axes[1,0].set_title('Precision vs Recall (Macro)')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Training time vs Accuracy\n",
        "axes[1,1].scatter(comparison_df['Training Time (s)'], comparison_df['Accuracy'],\n",
        "                  s=100, c=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    axes[1,1].annotate(model,\n",
        "                       (comparison_df['Training Time (s)'].iloc[i],\n",
        "                        comparison_df['Accuracy'].iloc[i]),\n",
        "                       xytext=(5, 5), textcoords='offset points')\n",
        "axes[1,1].set_xlabel('Training Time (seconds)')\n",
        "axes[1,1].set_ylabel('Accuracy')\n",
        "axes[1,1].set_title('Training Time vs Accuracy')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()# Visualize comprehensive comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0,0].bar(comparison_df['Model'], comparison_df['Accuracy'],\n",
        "              color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[0,0].set_title('Accuracy Comparison')\n",
        "axes[0,0].set_ylabel('Accuracy')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(comparison_df['Accuracy']):\n",
        "    axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# F1 Macro comparison\n",
        "axes[0,1].bar(comparison_df['Model'], comparison_df['F1 (Macro)'],\n",
        "              color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[0,1].set_title('F1 Score (Macro) Comparison')\n",
        "axes[0,1].set_ylabel('F1 Score (Macro)')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(comparison_df['F1 (Macro)']):\n",
        "    axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Precision vs Recall (Macro)\n",
        "axes[1,0].scatter(comparison_df['Recall (Macro)'], comparison_df['Precision (Macro)'],\n",
        "                  s=100, c=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    axes[1,0].annotate(model,\n",
        "                       (comparison_df['Recall (Macro)'].iloc[i],\n",
        "                        comparison_df['Precision (Macro)'].iloc[i]),\n",
        "                       xytext=(5, 5), textcoords='offset points')\n",
        "axes[1,0].set_xlabel('Recall (Macro)')\n",
        "axes[1,0].set_ylabel('Precision (Macro)')\n",
        "axes[1,0].set_title('Precision vs Recall (Macro)')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Training time vs Accuracy\n",
        "axes[1,1].scatter(comparison_df['Training Time (s)'], comparison_df['Accuracy'],\n",
        "                  s=100, c=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    axes[1,1].annotate(model,\n",
        "                       (comparison_df['Training Time (s)'].iloc[i],\n",
        "                        comparison_df['Accuracy'].iloc[i]),\n",
        "                       xytext=(5, 5), textcoords='offset points')\n",
        "axes[1,1].set_xlabel('Training Time (seconds)')\n",
        "axes[1,1].set_ylabel('Accuracy')\n",
        "axes[1,1].set_title('Training Time vs Accuracy')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()# Visualize comprehensive comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Accuracy comparison\n",
        "axes[0,0].bar(comparison_df['Model'], comparison_df['Accuracy'],\n",
        "              color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[0,0].set_title('Accuracy Comparison')\n",
        "axes[0,0].set_ylabel('Accuracy')\n",
        "axes[0,0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(comparison_df['Accuracy']):\n",
        "    axes[0,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# F1 Macro comparison\n",
        "axes[0,1].bar(comparison_df['Model'], comparison_df['F1 (Macro)'],\n",
        "              color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "axes[0,1].set_title('F1 Score (Macro) Comparison')\n",
        "axes[0,1].set_ylabel('F1 Score (Macro)')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(comparison_df['F1 (Macro)']):\n",
        "    axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Precision vs Recall (Macro)\n",
        "axes[1,0].scatter(comparison_df['Recall (Macro)'], comparison_df['Precision (Macro)'],\n",
        "                  s=100, c=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    axes[1,0].annotate(model,\n",
        "                       (comparison_df['Recall (Macro)'].iloc[i],\n",
        "                        comparison_df['Precision (Macro)'].iloc[i]),\n",
        "                       xytext=(5, 5), textcoords='offset points')\n",
        "axes[1,0].set_xlabel('Recall (Macro)')\n",
        "axes[1,0].set_ylabel('Precision (Macro)')\n",
        "axes[1,0].set_title('Precision vs Recall (Macro)')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Training time vs Accuracy\n",
        "axes[1,1].scatter(comparison_df['Training Time (s)'], comparison_df['Accuracy'],\n",
        "                  s=100, c=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
        "for i, model in enumerate(comparison_df['Model']):\n",
        "    axes[1,1].annotate(model,\n",
        "                       (comparison_df['Training Time (s)'].iloc[i],\n",
        "                        comparison_df['Accuracy'].iloc[i]),\n",
        "                       xytext=(5, 5), textcoords='offset points')\n",
        "axes[1,1].set_xlabel('Training Time (seconds)')\n",
        "axes[1,1].set_ylabel('Accuracy')\n",
        "axes[1,1].set_title('Training Time vs Accuracy')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m2rmE8leHJj"
      },
      "source": [
        "### Model Ranking Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gDqfyVnd-__"
      },
      "outputs": [],
      "source": [
        "print(\"üèÜ Best Models by Different Metrics:\")\n",
        "print(f\"1. Highest Accuracy: {comparison_df.iloc[0]['Model']} ({comparison_df.iloc[0]['Accuracy']:.4f})\")\n",
        "print(f\"2. Highest F1 (Macro): {comparison_df.sort_values('F1 (Macro)', ascending=False).iloc[0]['Model']} ({comparison_df.sort_values('F1 (Macro)', ascending=False).iloc[0]['F1 (Macro)']:.4f})\")\n",
        "print(f\"3. Fastest Training: {comparison_df.sort_values('Training Time (s)').iloc[0]['Model']} ({comparison_df.sort_values('Training Time (s)').iloc[0]['Training Time (s)']:.1f}s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hRZKa4peMqY"
      },
      "outputs": [],
      "source": [
        "# Best balanced model (accuracy vs speed)\n",
        "comparison_df['Efficiency'] = comparison_df['Accuracy'] / (comparison_df['Training Time (s)'] / 60)  # Accuracy per minute\n",
        "best_balanced = comparison_df.sort_values('Efficiency', ascending=False).iloc[0]\n",
        "print(f\"4. Best Balanced (Accuracy/Speed): {best_balanced['Model']} (Efficiency: {best_balanced['Efficiency']:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UgalnhBeQlb"
      },
      "outputs": [],
      "source": [
        "best_model = comparison_df.iloc[0]['Model']\n",
        "print(f\"‚úÖ **{best_model}** is the overall best model\")\n",
        "print(f\"   - Highest accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n",
        "print(f\"   - Good balance across all metrics\")\n",
        "print(f\"   - Suitable for production if inference time is not critical\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R58XJaxGhRUW"
      },
      "source": [
        "## Voting Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GffEQYQWr2pB"
      },
      "source": [
        "### Hard Voting\n",
        "\n",
        "---\n",
        "\n",
        "Implement hard voting ensemble that combines predictions from multiple models by taking the majority vote from class predictions. Each model provides one vote for the predicted class.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBGKUghqeVVM"
      },
      "outputs": [],
      "source": [
        "hard_start = datetime.now()\n",
        "hard_voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('svc', models['SVC']),\n",
        "        ('nb', models['Naive Bayes']),\n",
        "        ('rf', models['Random Forest']),\n",
        "        ('lr', models['Logistic Regression']),\n",
        "    ],\n",
        "    voting='hard',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7w2hAHRhfj6"
      },
      "outputs": [],
      "source": [
        "# Training Hard Voting Classifier\n",
        "print(\"Training Hard Voting Classifier...\")\n",
        "hard_voting_clf.fit(X_train_final, y_train_final)\n",
        "hard_time = (datetime.now() - hard_start).total_seconds()\n",
        "print(\"Hard Voting Classifier training finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIoZX_X1hj0K"
      },
      "outputs": [],
      "source": [
        "hard_voting_pred = hard_voting_clf.predict(X_test_tfidf)\n",
        "hard_voting_accuracy = accuracy_score(y_test, hard_voting_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models['Hard Voting'] = hard_voting_clf\n",
        "results['Hard Voting'] = {\n",
        "    'predictions': hard_voting_pred,\n",
        "    'accuracy': hard_voting_accuracy,\n",
        "    'precision': precision_recall_fscore_support(y_test, hard_voting_pred),\n",
        "    'training_time': hard_time,\n",
        "}"
      ],
      "metadata": {
        "id": "ll72PXGRpaWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2BZolfkrir_"
      },
      "outputs": [],
      "source": [
        "print(\"=== HARD VOTING CLASSIFIER RESULTS ===\")\n",
        "print(f\"Hard Voting Accuracy: {hard_voting_accuracy:.4f}\")\n",
        "print()\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, hard_voting_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model SVC for Soft\n",
        "\n",
        "---\n",
        "\n",
        "Prepare Support Vector Classifier model with probability estimation for soft voting, optimizing parameters to produce accurate prediction probabilities.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zMD2aFvsonIy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og1686MssUrY"
      },
      "outputs": [],
      "source": [
        "svc_prob_start = datetime.now()\n",
        "svc_model_prob = SVC(\n",
        "    C=1.0,\n",
        "    kernel='rbf',\n",
        "    gamma='scale',\n",
        "    class_weight='balanced',\n",
        "    random_state=42,\n",
        "    probability=True,\n",
        ")\n",
        "\n",
        "svc_model_prob.fit(X_train_final, y_train_final)\n",
        "svc_prob_time = (datetime.now() - svc_prob_start).total_seconds()\n",
        "svc_prob_pred = svc_model_prob.predict(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models['SVC Prob'] = svc_model_prob\n",
        "results['SVC Prob'] = {\n",
        "    'predictions': svc_model_prob,\n",
        "    'accuracy': accuracy_score(y_test, svc_prob_pred),\n",
        "    'precision': precision_recall_fscore_support(y_test, svc_prob_pred),\n",
        "    'training_time': svc_prob_time,\n",
        "}"
      ],
      "metadata": {
        "id": "pFHETJE8pI8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"  Accuracy: {results['SVC Prob']['accuracy']:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, svc_prob_pred))"
      ],
      "metadata": {
        "id": "vxjylOj5pMWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b51XY_Gkr46-"
      },
      "source": [
        "### Soft Voting\n",
        "\n",
        "---\n",
        "\n",
        "Implement soft voting ensemble that combines prediction probabilities from multiple models. This method typically provides better results as it considers the confidence level of each model.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0q_NXUn0rso3"
      },
      "outputs": [],
      "source": [
        "soft_start = datetime.now()\n",
        "soft_voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('svc', svc_model_prob),\n",
        "        ('nb', models['Naive Bayes']),\n",
        "        ('rf', models['Random Forest']),\n",
        "        ('lr', models['Logistic Regression']),\n",
        "    ],\n",
        "    voting='soft',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkQfTLB5r-dT"
      },
      "outputs": [],
      "source": [
        "# Training Soft Voting Classifier\n",
        "print(\"Training Soft Voting Classifier...\")\n",
        "soft_voting_clf.fit(X_train_final, y_train_final)\n",
        "soft_time = (datetime.now() - soft_start).total_seconds()\n",
        "print(\"Soft Voting Classifier training selesai!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QoERP-g5d0-"
      },
      "outputs": [],
      "source": [
        "soft_voting_pred = soft_voting_clf.predict(X_test_tfidf)\n",
        "soft_voting_accuracy = accuracy_score(y_test, soft_voting_pred)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models['Soft Voting'] = soft_voting_clf\n",
        "results['Soft Voting'] = {\n",
        "    'predictions': soft_voting_pred,\n",
        "    'accuracy': soft_voting_accuracy,\n",
        "    'precision': precision_recall_fscore_support(y_test, soft_voting_pred),\n",
        "    'training_time': soft_time,\n",
        "}"
      ],
      "metadata": {
        "id": "5-9zI_JNpsat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wty-90a9KU09"
      },
      "outputs": [],
      "source": [
        "print(\"=== SOFT VOTING CLASSIFIER RESULTS ===\")\n",
        "print(f\"Soft Voting Accuracy: {soft_voting_accuracy:.4f}\")\n",
        "print()\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, soft_voting_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY1WyL35K0k8"
      },
      "source": [
        "## Compare All Accuracy\n",
        "\n",
        "---\n",
        "\n",
        "Conduct comprehensive accuracy comparison of all implemented approaches: individual models, hard voting, and soft voting. This analysis determines the best approach for deployment.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JODAnfCmKp5B"
      },
      "outputs": [],
      "source": [
        "summary_data = []\n",
        "for model_name, result in results.items():\n",
        "    summary_data.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': result['accuracy'],\n",
        "        'Precision': result['precision'][0],\n",
        "        'Recall': result['precision'][1],\n",
        "        'F1-Score': result['precision'][2],\n",
        "        'Support': result['precision'][3],\n",
        "        'Training Time (s)': result['training_time'],\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "summary_df = summary_df.sort_values(by='Accuracy', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_uzJtI9HLQpO"
      },
      "outputs": [],
      "source": [
        "print(\"=== FINAL MODEL COMPARISON ===\")\n",
        "print(summary_df.to_string(index=False))\n",
        "print()\n",
        "print(\"Best Model:\", summary_df.iloc[0]['Model'])\n",
        "print(\"Best Accuracy:\", summary_df.iloc[0]['Accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1mRkAv0MdNe"
      },
      "source": [
        "### Visualization Model Comparison\n",
        "\n",
        "---\n",
        "\n",
        "Create final visualizations displaying performance comparisons of all models and ensemble methods in an easily understandable format, such as bar charts or radar charts for various evaluation metrics.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe-l33quLRhl"
      },
      "outputs": [],
      "source": [
        "models = summary_df['Model'].tolist()\n",
        "accuracies = summary_df['Accuracy'].tolist()\n",
        "\n",
        "accuracies = [round(acc, 4) for acc in accuracies]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1RcE0FkMigI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#592E83', '#1B998B']\n",
        "bars = plt.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, accuracy in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005,\n",
        "             f'{accuracy:.4f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.title('Model Performance Comparison - Traveloka Sentiment Analysis',\n",
        "          fontsize=16, fontweight='bold', pad=20)\n",
        "plt.xlabel('Models', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "plt.ylim(0.75, 0.87)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing SVC"
      ],
      "metadata": {
        "id": "0vYRkuz4nrFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_train_ros, y_train_ros = ros.fit_resample(X_train_tfidf, y_train)"
      ],
      "metadata": {
        "id": "OJyY5cQknvgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model_ln = SVC(kernel='linear', random_state=42)\n",
        "svm_model_ln.fit(X_train_ros, y_train_ros)\n",
        "\n",
        "y_pred = svm_model_ln.predict(X_test_tfidf)\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "print(\"Precission:\")\n",
        "print(precision_recall_fscore_support(y_test, svc_final_pred),)"
      ],
      "metadata": {
        "id": "aPIA7PAHn6B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_final_start = datetime.now()\n",
        "svc_final = SVC(\n",
        "    C=0.1,\n",
        "    kernel='rbf',\n",
        "    gamma='scale',\n",
        "    class_weight={0:1, 1:5, 2:1},\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "svc_final.fit(X_train_tfidf, y_train)\n",
        "svc_final_time = (datetime.now() - svc_final_start).total_seconds()\n",
        "svc_final_pred = svc_final.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "KcBe2g4ouK1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc_final_acc = accuracy_score(y_test, svc_final_pred)\n",
        "print(f\"  Accuracy: {svc_final_acc:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, svc_final_pred))\n",
        "print(\"Precission:\")\n",
        "print(precision_recall_fscore_support(y_test, svc_final_pred),)\n"
      ],
      "metadata": {
        "id": "nxoz0qqOuMdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "---\n",
        "    \n",
        "Implement inference functions to perform sentiment prediction on new data using the best selected model. This section includes a complete preprocessing pipeline from raw text to final prediction results.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jph_mPhip4XK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "reiyN9HhTN8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(review_text, svc_model, tfidf_vectorizer, label_encoder):\n",
        "    # Preprocess the text\n",
        "    cleaned_text = preprocess_text(review_text)\n",
        "\n",
        "    # Transform text using the same TF-IDF vectorizer\n",
        "    text_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = svc_model.predict(text_tfidf)[0]\n",
        "\n",
        "    # Calculate confidence score based on the decision function\n",
        "    decision_scores = svc_model.decision_function(text_tfidf)[0]\n",
        "\n",
        "    # For multi-class classification, get the score of the predicted class\n",
        "    if len(decision_scores.shape) > 1:  # If one-vs-rest (shape: [n_samples, n_classes])\n",
        "        confidence_score = decision_scores[prediction]\n",
        "    else:  # If one-vs-one (shape: [n_samples * (n_classes -1)/2])\n",
        "        # For OVO, we need to normalize the scores\n",
        "        confidence_score = np.max(decision_scores)\n",
        "\n",
        "    # Convert to percentage using sigmoid\n",
        "    confidence = 1 / (1 + np.exp(-np.abs(confidence_score))) * 100\n",
        "\n",
        "    # Convert prediction to original label\n",
        "    sentiment_label = label_encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "    return {\n",
        "        'text': review_text,\n",
        "        'predicted_sentiment': sentiment_label,\n",
        "        'confidence': round(float(confidence), 1),\n",
        "    }"
      ],
      "metadata": {
        "id": "vo8GqWnwTOoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment_with_proba(review_text, svc_model, tfidf_vectorizer, label_encoder):\n",
        "\n",
        "    # Preprocess the text\n",
        "    cleaned_text = preprocess_text(review_text)\n",
        "\n",
        "    # Transform text using the same TF-IDF vectorizer\n",
        "    text_tfidf = tfidf_vectorizer.transform([cleaned_text])\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = svc_model.predict(text_tfidf)[0]\n",
        "    prediction_proba = svc_model.predict_proba(text_tfidf)[0]\n",
        "\n",
        "    # Convert prediction back to original label\n",
        "    sentiment_label = label_encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "    # Get confidence score\n",
        "    confidence = max(prediction_proba) * 100\n",
        "\n",
        "    return {\n",
        "        'text': review_text,\n",
        "        'predicted_sentiment': sentiment_label,\n",
        "        'confidence': confidence,\n",
        "        'probabilities': {\n",
        "            'negative': prediction_proba[0] * 100,\n",
        "            'neutral': prediction_proba[1] * 100,\n",
        "            'positive': prediction_proba[2] * 100\n",
        "        }\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Egp9aJvATc2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews = [\n",
        "      \"Aplikasi Traveloka sangat bagus dan mudah digunakan!\",\n",
        "      \"Pelayanan buruk, aplikasi sering error dan lambat\",\n",
        "      \"Aplikasi lumayan, tidak terlalu bagus tidak terlalu buruk\",\n",
        "      \"Booking hotel mudah dan cepat, recommended!\",\n",
        "      \"Aplikasi jelek banget, selalu crash\"\n",
        "  ]"
      ],
      "metadata": {
        "id": "4Npqvja8XtA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìù SAMPLE REVIEWS & PREDICTIONS:\")\n",
        "print()\n",
        "\n",
        "for i, review in enumerate(sample_reviews, 1):\n",
        "    result = predict_sentiment_with_proba(review, svc_final, tfidf_vectorizer, label_encoder)\n",
        "\n",
        "    print(f\"{i}. Review: '{review}'\")\n",
        "    print(f\"   Predicted: {result['predicted_sentiment']} ({result['confidence']:.1f}% confidence)\")\n",
        "    print(f\"   Probabilities: Neg={result['probabilities']['negative']:.1f}%, \"\n",
        "          f\"Neu={result['probabilities']['neutral']:.1f}%, \"\n",
        "          f\"Pos={result['probabilities']['positive']:.1f}%\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "8Xg7OVPeS5co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save TF-IDF vectorizer\n",
        "# joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
        "# print(\"‚úÖ TF-IDF Vectorizer saved: tfidf_vectorizer.pkl\")"
      ],
      "metadata": {
        "id": "ykcgosRiYLIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save label encoder\n",
        "# joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "# print(\"‚úÖ Label Encoder saved: label_encoder.pkl\")"
      ],
      "metadata": {
        "id": "8nDKfaDuZYgV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AZqhZEW0nqD0",
        "l2SXAA4an57y",
        "TSAiEgMM5ddO",
        "tOxvfjwJuW9R",
        "2uzv66pfufVF",
        "h2-6fjjJG81N",
        "NzO6ZVhoK_xl",
        "RBiVQ9XkLTm1",
        "Md4kzDgTMXCH",
        "IyLx2swCQ5rM",
        "A8ETq2rdRdtn",
        "VYfyu5HTSWRU",
        "JJ1EUu5pTnL2",
        "mhmJoafpZ4Mc",
        "LCkL8uFNd0tT",
        "3m2rmE8leHJj",
        "GffEQYQWr2pB",
        "zMD2aFvsonIy",
        "b51XY_Gkr46-",
        "UY1WyL35K0k8",
        "c1mRkAv0MdNe",
        "jph_mPhip4XK"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}